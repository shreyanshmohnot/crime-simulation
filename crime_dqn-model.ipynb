{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Model for Reinforcement Learning\n",
    "\n",
    "from collections import deque\n",
    "from gym.spaces import Box\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from CrimeWorld import CrimeWorld\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size[0]\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(4, input_shape=(self.state_size,self.state_size,1), kernel_size=(4,4), strides=(2,2), padding='valid', activation='relu'))\n",
    "        model.add(Conv2D(8, kernel_size=(4,4), strides=(2,2), padding='valid', activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # initialize the crime world\n",
    "    env = CrimeWorld()\n",
    "    initial_policeX = 60\n",
    "    initial_policeY = 60\n",
    "    # adding agent into the world\n",
    "    env.add_agent(initial_policeX,initial_policeY)\n",
    "\n",
    "    # current state size for the DQN\n",
    "    police_state_size = env.get_state().shape\n",
    "\n",
    "    # number of actions to perform by the agent\n",
    "    action_size = 5\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    \n",
    "    # file to save the results\n",
    "    with open(\"results_episodes\", mode='w+') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        writer.writerows([[\"episode\",\"policeX\",\"policeY\",\"action\",\"reward\",\"new_B\",\"new_n\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize the DQN and build the model\n",
    "    agent = DQNAgent(police_state_size, action_size)\n",
    "\n",
    "    # saving each episode data\n",
    "    save_episode = []\n",
    "    for e in range(EPISODES):\n",
    "        # reset the world to initial conditions and matrices\n",
    "        state = env.reset()\n",
    "\n",
    "        state = np.reshape(state, (1, police_state_size[0], police_state_size[1],1))\n",
    "        for time_step in range(1000):\n",
    "            # generate an action by the model for getting the reward from the environment\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # get the reward and future possible rewards to the agent \n",
    "            reward, next_state, done = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, (1, police_state_size[0],police_state_size[1],1))\n",
    "\n",
    "            # agent remembers its previous steps to generate and predict the future possibilities\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # saving every time step data to feed into the episodes\n",
    "            save_episode.append([e, env.policeX, env.policeY , action, reward, env.result_matrixsum(action, reward)[0], env.result_matrixsum(action, reward)[1]])\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "            \n",
    "            # for george: video code\n",
    "            # if(e==99):\n",
    "                # policeX_axis.append(env.policeX)\n",
    "                # policeY_axis.append(env.policeY)\n",
    "                # plt.imshow(env.B)\n",
    "                # plt.plot(policeY_axis, policeX_axis, color='green')\n",
    "                # plt.gca().invert_yaxis()\n",
    "                # plt.savefig(\"images/\" + str(time_step) + '.png')\n",
    "                # display.clear_output(wait=True)\n",
    "                # display.display(plt.gcf())\n",
    "\n",
    "        # saving every episode for data prediction and generalize the model\n",
    "        if(e%20==19):\n",
    "            with open(\"results_episodes\", mode='a') as csv_file:\n",
    "                writer = csv.writer(csv_file, delimiter=',')\n",
    "                writer.writerows(save_episode)\n",
    "            save_episode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
